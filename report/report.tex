\documentclass[a4paper]{article}

\usepackage[UKenglish]{babel}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[T1]{fontenc}
\usepackage{listings}

\AtBeginDocument{\renewcommand{\abstractname}{Abstract}}

\begin{document}
\begin{titlepage}
	\begin{center}
	\textsc{\LARGE Machine Learning in Practice\\}
	\textsc{\Large Report}\\[1.5cm]
	\includegraphics[height=100pt]{logo}
   
	\vspace{0.4cm}
	\textsc{\Large Radboud University Nijmegen}\\[.5cm]
	\hrule
	\vspace{0.4cm}
	\textbf{\huge Acquire Valued Shoppers Challenge}\\[0.4cm]
	\textbf{\huge Team Shopper-Hivemind}\\[0.4cm]
	\hrule
	\vspace{2cm}
	\begin{minipage}[t]{0.45\textwidth}
	\begin{flushleft} \large
	Wouter Geraerdts\\
	sXXXXXX\\[0.7cm]
	Matthijs Hendriks\\
	s4068459\\[0.7cm]
	\end{flushleft}
	\end{minipage}
	\begin{minipage}[t]{0.45\textwidth}
	\begin{flushright} \large
	Thomas N\"agele\\
	s4031253\\[0.7cm]
	Mathijs Vos\\
	s4024443\\[0.7cm]
	\end{flushright}
	\end{minipage}
	\vspace{.7cm}
	
	\begin{abstract}
		ABSTRACT HIERZO
	\end{abstract}
	\vspace{.7cm}

	{\large 3 July 2014}
	\vfill
	\end{center}

\end{titlepage}

\newpage

\section{Introduction}
For this final assignment for Machine Learning in Practice, we chose the \emph{Acquire Valued Shoppers Challenge} from Kaggle\footnote{https://www.kaggle.com/c/acquire-valued-shoppers-challenge}. The goal for this competition is to find out which shoppers will become regular buyers of a certain product after receiving a personalized coupon offer. This data is useful for stores so they can make a relavant offer to their clients at the checkout.

The dataset contains a few tables which contain at least one year of transactions for each client before making an offer to them. It is therefore possible to view a complete buyers history of one client based on his or her transactions. There are also trainHistory file which contain information about an offer as presented to a customer including the outcome: whether this client has become a regular buyer or not. An offers file gives you more information about the offers: the company, department etc. are stored there. For every customer there is at least one year of transactions given, recorded before an offer was made. Transaction information includes a product category, company, department and brand as well a information about the product that is bought, such as the size, quantity and amount.

We have chosen this competition because we wanted to experiment what it was like to work with a dataset of this size and because it also has some very common real-life applications. The challenge of processing 22 GB of data, which doesn't fit in our RAM anymore would be the biggest. It is also a nice idea that there is quite some money to win in this competition.

\section{Approach}


\section{Features}


\section{Classification}
\subsection{Algorithms}
For this project we've experimented with a number of different (variants of) algorithms. The two `main' algorithms we've been looking at are Support Vector Machines and Vowpal Wabbit. Our experiences with the latter weren't that good. Although some contestants on Kaggle say that they have got some convincing results out of it (and very fast, since Vowpal Wabbit has a run time linear in the number of data points), we haven't been able to get it to work at all. We feel the Wabbit-library is still very `academic', by which we mean it is hard to install and configure the library and to get it to work with your own code. This did not really convince us of the quality of the library. We then decided to leave it for what it was and continue with libSVM and libLinear.
\subsection{Implementation}
We then moved on and continued working on classification with Support Vector Machines. The specific implementations that we have used are libSVM and libLinear, which are both widely used libraries for SVM. The former provides a variety of different SVM-kernels and methods, whereas the latter only supports Support Vector Machines with linear kernels, though more optimised than the linear kernel found in libSVM.

With libSVM we use the sparse array representation of the data, meaning that we leave each element that is equal to zero out. According to the documentation of livSVM, this allows for faster processing of the data. What's not mentioned in the documentation is that this is hard to get right. It caused a memory-bug in our implementation, which then caused libSVM to just output \emph{NaN} everywhere. However, once this was fixed we got a more promising output.

We used libSVM with the \emph{C\_SVC} SVM-type and an \emph{RBF}-kernel. The value of \emph{C} was $10$, and we did not use the built-in shrinking methods.

By default, libSVM only assigns a (binary) class to each data point that you test. However, for this project we not only needed to indicate if a client would become a repeat buyer, but we needed to give a \emph{probability} that the client would return. Luckily, libSVM offers method to generate a model with probability information during training. Unfortunately, this is really slow. As a compromise between this, we do not ask libSVM to build a model with probabilities. Instead, we ask for the internal decision value and use that instead of a real probability. Although these are different numbers, they give the same \emph{area under the curve} and thus this would not influence our score.

\subsection{Performance}
First of all, libSVM is really slow. Of course we are dealing with a huge data set here. Training the data model however takes the whole night, and testing $150000$ data points takes almost two hours. We tried to run the same analysis, but then using libLinear instead of libSVM. LibLinear is an optimised version of the linear kernel for support vector machines. It was made by the same authors as libSVM, and has a similar API.

And indeed, the performance was better: libLinear trained a model in roughly one hour, and testing the same $150000$ data points only took $x$ minutes. However when we submitted the resulting output to Kaggle, the accuracy of our estimation was lower than with libSVM.

\section{Results}


\section{Conclusions}


\end{document}
