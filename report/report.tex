\documentclass[a4paper]{article}

\usepackage[UKenglish]{babel}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[T1]{fontenc}
\usepackage{listings}

\AtBeginDocument{\renewcommand{\abstractname}{Abstract}}

\begin{document}
\begin{titlepage}
	\begin{center}
	\textsc{\LARGE Machine Learning in Practice\\}
	\textsc{\Large Report}\\[1.5cm]
	\includegraphics[height=100pt]{logo}
   
	\vspace{0.4cm}
	\textsc{\Large Radboud University Nijmegen}\\[.5cm]
	\hrule
	\vspace{0.4cm}
	\textbf{\huge Acquire Valued Shoppers Challenge}\\[0.4cm]
	\textbf{\huge Shopper-Hivemind}\\[0.4cm]
	\hrule
	\vspace{1cm}
	\begin{minipage}[t]{0.45\textwidth}
	\begin{flushleft} \large
	Wouter Geraerdts\\
	sXXXXXX\\[0.7cm]
	Matthijs Hendriks\\
	s4068459\\[0.7cm]
	\end{flushleft}
	\end{minipage}
	\begin{minipage}[t]{0.45\textwidth}
	\begin{flushright} \large
	Thomas N\"agele\\
	s4031253\\[0.7cm]
	Mathijs Vos\\
	s4024443\\[0.7cm]
	\end{flushright}
	\end{minipage}
	\vspace{.7cm}
	
	\begin{abstract}
		We participated in Kaggle's competition \emph{Acquire Valued Shoppers Challenge} for the last assignment of Machine Learning in Practice, at the Radboud University Nijmegen. In this competition you are asked to provide a classification algorithm that is able to predict whether a customer of some store will become a repeat buyer after receiving and redeeming an offer. To solve this challenging problem, we used Support Vector Machines, implemented in \emph{libSVM}, and \emph{libLINEAR}, a linear classification library. We found that the former performs not so well in terms of computation time, but does beat the much faster linear classification algorithm of \emph{libLINEAR} in terms of accuracy. At the time of writing, we do not know our accuracy on all of the test data (Kaggle does not publish these results before the competition has ended), but we scored $0.56619$ on approximately 20\% of the test data.
	\end{abstract}
	\vspace{.7cm}

	{\large 3 July 2014}
	\vfill
	\end{center}

\end{titlepage}

\newpage

\section{Introduction}
For this final assignment for Machine Learning in Practice, we chose the \emph{Acquire Valued Shoppers Challenge} from Kaggle\footnote{https://www.kaggle.com/c/acquire-valued-shoppers-challenge}. The goal for this competition is to find out which shoppers will become regular buyers of a certain product after receiving a personalized coupon offer. This data is useful for stores so they can make a relavant offer to their clients at the checkout.

The dataset contains a few tables which contain information about the previous transactions and the offers. There is also trainHistory file which contain information about an offer as presented to a customer including the outcome: whether this client has become a regular buyer or not. An offers file gives you more information about the offers: the company, department etc. are stored there. For every customer there is at least one year of transactions given, recorded before an offer was made, which allows us to have a complete image of the buyers history. Transaction information includes a product category, company, department and brand as well a information about the product that is bought, such as the size, quantity and amount.

We have chosen this competition because we wanted to experiment what it was like to work with a dataset of this size and because it also has some very common real-life applications. The challenge of processing 22 GB of data, which doesn't fit in our RAM anymore would be the biggest. It is also a nice idea that there is quite some money to win in this competition.

\section{Approach}


\section{Features}
We divided feature generation in two chunks: feature extraction and feature normalisation. The former stage gives us a big matrix containing 227 features per client, whilst the latter transforms these features into a format the libraries we use can use.

\subsection{Feature Extraction}
To extract useful features, we need to understand the way the data set is built up and how each of the three files (\texttt{history}, \texttt{transactions}, and \texttt{offers}) related to one another. Each client has a single history file, and all clients have received a single offer. Next to this, we have at least a year of shopping history per client. Since client id's do not have to be unique over all store chains, each transaction can be linked to the client that did the transaction by the client id and the store chain id. We assume that a client does not go to the same store chain multiple times on a single day, which allows us to describe the set of items bought by a client on the same day as a `basket'. The number of baskets equals the number of trips the client has made. Unfortunately, the data set does contain some noise. For example, there are cases where the number of items bought is positive, but the value of these items negative, or the other way around. During feature extraction we ignore these inconsistencies and pretend as if the entry does not exist.

All of our features can be roughly divided in four categories. We have the offer-category, the purchases-category, the ratio-category, and the faithfulness-category.

The former of the four contains all features that are directly related to the offer a client gets. In our case, we have chosen to use only the offer value and the offer quantity. These two values are the only two continuous values of the offer. The other ones, the category, company, and brand may be relevant to whether or not a client becomes a repeat buyer after redeeming the offer, but since these are absolute values (id's), we have chosen to use these attributes only in relation to the client's purchase history.

A client's purchase history is the most powerful `tool' we could use to generate features. It contains a detailed description of what the client has purchased and returned the last period, and when he or she has. We used this history to generate several bins of features. Each bin contains the same features, except for an other period. The first bin contains the purchases and returns of a client for the last month, second one for the last two months, and so on. After we have reached the last half a year of the client's purchases and returns, we add the same data for the last year, and for `ever'. This gives us a total of 8 bins per client. As for the features per bin: each bin contains the number of items that a client has bought and returned, that meet a certain condition. This conditions are related to the offer the client has received; we map `parts' of the offer to the client's history and create our conditions based on that: ``has the client ever bought something of the brand of the offer?'' Or ``has the client ever bought something at this company?'' We combine these attributes to get a total of eleven features: \emph{brand}, \emph{category}, \emph{company}, \emph{department}, \emph{brand} and \emph{category}, \emph{brand} and \emph{company}, \emph{brand} and \emph{department}, \emph{brand} and \emph{company} and \emph{category}, \emph{brand} and \emph{company} and \emph{department}, \emph{company} and \emph{category}, and \emph{company} and \emph{department}. To these features, we add one more feature, being the number of times the client has bought (c.q. returned) anything at all. This gives use a total of twelve features, for both purchases and returns, per bin. With 8 bins per client, this makes a total of $8\cdot24=192$ features. Note that not all possible combinations are included per bin. This is due to the fact that some combinations are rather senseless. For example, anything requiring the product to be of both the offer product's category and department. Since one category is always of the same department, this would not provide us with any new information. Also note that we, initially, only know the brand, the category, and the company of the offer. We do not know the department, but if the client has ever bought the offer's product before, we can figure out the department of the offer.

The eleven features that we get from checking whether a client has bought an item that meats one of the twelve constraints, are reused when calculating ratios. These ratios are useful to determine if a client buys the same (type of) item regularly, or if the client buys at the same company regularly, or if one of those (twelve) things differ from day to day, so to speak. We find the totals as described above, and divide these by the total number of times a client has bought anything at all. Thereby obtaining ratios, which we use as extra features.

The last set of features contains the `faithfulness' of a client within a given context, i.e. the number of $X$ per $Y$. This is an attempt to find out if a client can be pulled towards a certain offer of a certain company. For example, assume a client goes to two different companies. At one of these companies, he usually buys a very diverse set of products, a set covering many brands and categories. Yet, at the other company, he always purchases a product in the same category, or the same department, or of the same brand. The latter company might find that interesting to know, as such a client is more likely to redeem an offer that fits in his regular purchase pattern. For these features, we look at the number of: \emph{brands}, \emph{companies}, and \emph{categories} per \emph{department}, \emph{departments}, \emph{companies}, and \emph{categories} per \emph{brand}, \emph{brands}, \emph{companies}, and \emph{categories} per \emph{company}, and \emph{brands} and \emph{companies} per category. Again, we have left out redundant combinations. We count this for both purchases and returns, giving us another 22 features to use. 

\subsection{Feature Normalisation}
The libraries we use do not just accept any values in the feature sets, and some features will only make the classifying library slower. To prevent this, we transform our features. First, we remove all features that have the same value for every customer. For most features this is not the case, but some are. For example, as we were engineering our features, we found that none of the test clients has bought a product in the month before receiving the offer. By removing these features, we prevent further complications. The next thing we do is normalising all features. This is required as libSVM only accepts continuous features of which each value is in the range $[0,1]$. We normalise by dividing each feature value by the highest value any client has for that feature. This leaves us with a set of features most classification libraries will accept.

\section{Classification}
\subsection{Algorithms}
For this project we've experimented with a number of different (variants of) algorithms. The two `main' algorithms we've been looking at are Support Vector Machines and Vowpal Wabbit. Our experiences with the latter weren't that good. Although some contestants on Kaggle say that they have got some convincing results out of it (and very fast, since Vowpal Wabbit has a run time linear in the number of data points), we haven't been able to get it to work at all. We feel the Wabbit-library is still very `academic', by which we mean it is hard to install and configure the library and to get it to work with your own code. This did not really convince us of the quality of the library. We then decided to leave it for what it was and to continue with libSVM and libLinear.
\subsection{Implementation}
We then moved on and continued working on classification with Support Vector Machines. The specific implementations that we have used are libSVM and libLinear, which are both widely used libraries for SVM. The former provides a variety of different SVM-kernels and methods, whereas the latter only supports Support Vector Machines with linear kernels, though more optimised than the linear kernel found in libSVM.

With libSVM we use the sparse array representation of the data, meaning that we leave each element that is equal to zero out. According to the documentation of livSVM, this allows for faster processing of the data. What's not mentioned in the documentation is that this is hard to get right. It caused a memory-bug in our implementation, which then caused libSVM to just output \texttt{NaN} everywhere. However, once this was fixed we got a more promising output.

We used libSVM with the \emph{C\_SVC} SVM-type and an \emph{RBF}-kernel. The value of \emph{C} was $10$, and we did not use the built-in shrinking methods.

By default, libSVM only assigns a (binary) class to each data point that you test. However, for this project we not only needed to indicate if a client would become a repeat buyer, but we needed to give a \emph{probability} that the client would return. Luckily, libSVM offers method to generate a model with probability information during training. Unfortunately, this is really slow. As a compromise between this, we do not ask libSVM to build a model with probabilities. Instead, we ask for the internal decision value and use that instead of a real probability. Although these are different numbers, they give the same \emph{area under the curve} and thus this would not influence our score.

\subsection{Performance}
First of all, libSVM is really slow. Of course we are dealing with a huge data set here. Training the data model however takes the whole night, and testing $150000$ data points takes almost two hours. We tried to run the same analysis, but then using libLinear instead of libSVM. LibLinear is an optimised version of the linear kernel for support vector machines. It was made by the same authors as libSVM, and has a similar API.

And indeed, the performance was better: libLinear trained a model in roughly one hour, and testing the same $150000$ data points only took $x$ minutes. However when we submitted the resulting output to Kaggle, the accuracy of our estimation was lower than with libSVM.


\section{Results \& Discussion}
With LibSVM we scored 0.56619 on Kaggle. Using LibLinear we only reached 0.52422. The training data is read in 5 minutes, after which calculation of all the features takes 20 minutes. The size of the featurematrix results in 124 MiB.

\vspace{1em}

It's a pitty we didn't score very well on the leaderboard, but it means that there is plenty of room for improvement. First of all, we did not have the time left in the end to tune the parameters of our used machine learning methods for better performance. We probably could have scored better if we tuned these parameters a bit.

Secondly, we chose to develop our program in C++ with Qt, which caused quite some delay. Not only because not everyone in our group was familiar with C++/Qt, but also because we needed to write a lot of code for ourself and lost a lot of time getting libraries to work on our systems. We chose C++/Qt because we wanted to implement a fast way of handling such a large dataset, but maybe we could have better used pythons machine learning tools at the cost of some performance. These tools are used a lot by the machine learning community and are known to work with all kinds of algorithms and datasets. It wouldn't have cost this much work to get everything to work and would've given us more time to optimize the machine learning part.

Instead of using python, as suggested above, we also could've used our feature extraction toolchain in C++ and use CPython for the machine learning part, so we could use the python machine learning tools. Storing the training set into a new file after processing it using our C++ tool was also possible: this could be read using a standard python tool.


\end{document}
